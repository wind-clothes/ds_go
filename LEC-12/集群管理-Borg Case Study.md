## 集群管理-Borg Case Study

阅读：“谷歌与Borg的大规模集群管理”，Abhishek Verma，
Luis Pedrosa，Madhukar Korupolu，David Oppenheimer，Eric Tune和John Wilkes。
第十届欧洲计算机系统会议论文集（EuroSys 2015）。

我们为什么看这篇文章？
  复杂的，真实的分布式系统，包含关键的6.824概念
    依赖于Paxos（类似于RAFT，实验室2和Zookeeper）
  说明您的分布式应用程序将如何实际运行
    例如MapReduce（实验室1），分布式KV存储（实验室4）
  随着应用程序日益依赖于在线后端，集群是常见的
    许多公司经营自己的服务或分析数据
    一些问题需要他们（大数据，大型机器学习）
  自动化集群管理器现在是“主流”
    你可能已经使用了一个，或将来会做
    整齐的系统技巧来增加资源利用率=>节省真钱！

Google Borg
  在Google开发内部用于大型共享集群
    自动化大型群集上的应用程序部署和资源管理
    支持几乎所有的Google工作负载，包括“云”业务，MapReduce
  启发了几个类似的开源系统
    Apache Mesos
    Google Kubernetes
    Docker群
    Hashicorp游牧人
  行业热门话题（“编排”）

动机：资源共享（“合并”）省钱
  需要许多服务器同时为数千个用户提供服务
    必须规定峰值负荷
    但大多数时候，加载“高峰”
    因此，机器使用不足（例如，夜间5％的CPU负载）
  但是：通常也具有低优先级的工作负载
    可以使用这些填充槽来利用
    因此，总体上减少机器
  [图：2x资源使用时间表，突出松散资源]
  在数千台机器的规模上，即使是小的效率也能节省$ MM

一个解决方案：只需在一台时间共享的机器上运行进程
  Developer / sysadmin手动放置应用程序
    要启动服务，请安装并设置init以在机器启动时启动它
    要开始批处理数据处理，请登录，安装程序并启动
  问：这种方法有什么问题？
    安全性差
      许多用户都有ssh登录到机器
      ...但不能给每个人根（=安装的东西是痛苦的）
      文件系统，进程列表等，共享=>信息泄漏
    方便性差
      共享文件系统和库：版本/依赖性混乱
      用户/开发人员必须了解如何和何处部署应用程序
    效率差
      需要人工人员配置，没有任何自动化
      一旦将服务/应用程序部署到机器，它就是静态的

Borg：一个自动化的集群管理器
  设计目标：
    隐藏资源分配和故障处理的详细信息
    非常高的可靠性和可用性
    高效地执行10,000多台机器上的工作负载
    支持*所有*工作负载
      存储服务器（GFS，BigTable，MegaStore，Spanner）
      生产前端（GMail，Docs，网页搜索）
      批量分析和处理（贴图，抓取，索引更新）
      机器学习（Brain / TensorFlow）
  挑战：
    多少控制和权力暴露给用户？
    如何有效地将工作包装到机器上？
    如何应对故障（机器或Borg）和网络分区？
    如何隔离不同用户的工作负载？

博格生命周期概述
  [图：可视化生命周期，参见 图2在论文中]
  1.用户（Google工程师）提交作业，其中包含一个或多个任务
  博格决定放置任务的位置
  任务开始运行，Borg监控他们的健康
  4.处理运行时事件
    4a）任务可能再次“等待”（由于机器故障，抢占）
    4b）博格收集监控信息
  5.完成任务，或按用户请求删除作业
  Borg清理状态，保存日志等
  （这忽略了分配，但将它们看作是组合其他任务的较大“任务”）。

Borg系统组件
  大多数概括为其他集群管理器
  比照 图1在纸上
  [图：图1的简化版]
  BorgMaster
    中央“大脑”：保持集群状态，复制可靠性（Paxos）
  Borglet
    每机器代理，监督本地任务并与BorgMaster进行交互
  调度
    决定放置哪些机器任务（稍后）
  前端
    CLI / BCL / API：编程作业提交，配置说明
    仪表板：监控服务SLA（例如，尾部延迟）
    Sigma UI：内省（“为什么我的任务待定？”）

可靠性
  复制状态在BorgMaster
    一个主动（读写）小学（通过Paxos选出）
    主要跟随四个备份（只读）
      每个副本在内存和磁盘上都有状态（基于Paxos的存储）
    在主要超时时，领导选举通过Paxos / Chubby进行
      需要10秒到1分钟
      停电后，副本加入作为备份并与其他人同步
    检查点启用调试和快速恢复
      从一开始就不用重播日志
  Borglet：本地机器代理
    定期与BorgMaster联系，检查健康状况并通过指导
      以投票为基础，以避免意外的BorgMaster DoS
    必须处理从BorgMaster断开连接（BM故障，网络分区）
      所有任务都会继续运行（以防止所有BorgMaster失败）
      一旦恢复，杀死重新安排在其他地方的任务
  问：网络分区会发生什么？几种选择（不在纸上）
     - 需要所有BorgMaster replicas =>的共识，好像都失败了
     - 多数分区选举领导，持卡车; 少数分区
        像所有BorgMasters都失败一样

集群
  物理数据中心（建筑）拥有多个“集群”
  每个集群被细分为逻辑“单元格”
  Borg管理一个单元格，用户必须自己选择要使用哪个单元格
    地理分布，所以工作经常复制

工作量
  长时间运行的服务
    经常为用户（或其他作业）请求提供服务：守护进程
    从不“完成”：仅由于故障或作业终止/升级而终止
    通常（但不总是）高优先级和延迟敏感
    严格的容错要求（扩展到故障域）
  批处理作业
    努力实现有限目标，完成后终止
    对短期业绩波动不敏感
  优先的概念
    生产（“监控”+“生产”）
      保证运行并始终收到资源
    非生产（“批”+“尽力”）
      二等公民
  示例[diagram / grid]
    prod /服务：BigTable，GMail前端服务器，模型服务
    prod / Batch：GFS数据迁移
    批/服务： - 
    批/批：机器学习模式训练，数据转换
    最大努力/服务：测试部署新版本的服务
    最大努力/批次：实习MapReduce工作，探索性ML模型培训

入场控制
  必须有“配额”提交作业
  实际上是一个人与环的公平制度
  配额费用（从团队预算），所以超额费用是昂贵的
    除了“低质量”的资源外
      从HW的角度来看并不逊色
      但是可以被抢占或扼制（并且会更频繁地）
    鼓励使用低优先级（“免费”）
  期限内购买配额（如按月计算）
    “日历”
    主要用于prod / Service作业
  不同于每个任务的资源请求！
    配额是每用户/每个团队
    Qutoa进行中期能力规划

调度
  集群管理的关键部分：每个任务去哪里？
    影响容错
    影响机器负荷
    影响用户等待时间（直到发现地点）
  可行性检查：哪些机器是候选人？
    必须有足够的可用资源
      在各个方面：CPU，内存，磁盘空间，I / O带宽
      “足够”的概念取决于任务优先级（prod / non-prod不同）
    考虑放置约束
      硬：必须满足（例如，“机器必须有公共IP”，“GPGPU”）
      软：偏好，不能满足（例如，“更快的处理器”）
      用于确保容错的硬约束 
  得分：哪些候选机是最好的选择？
    软约束影响分数
    最大限度地减少干扰：减少抢占任务的次数和优先级
    较高分数=更好的包装

利用/包装
  问：为什么这是一个问题？估计容易浪费资源+人类不好
  浪费的原因
    用户要求比他们需要更多的资源
    所有内存保留，但大量的CPU免费
    负载平衡良好，但没有大的任务空间
    任务的需求随时间而变化（例如，日间服务负载）
  纸张经过许多设计选择，导致更好的包装
    但是通过反向积极评价：如果没有做出这些选择，则更糟糕

集群压缩度量
  用于本文中的许多图形，但有些不寻常
  我们如何评估调度程序的工作有多好？
    可以使用利用率，但是对于浪费几乎没有
    可以测量不可用的“洞”，但孔的可用性是任务依赖的
    可以大大增加工作量，但复杂化使现实
  关键问题：“我可以离开多少小集群？”
    ...没有用尽能力（许多待处理的任务）
    如果调度程序包更紧密，则一个较小的集群仍然可以工作
    答案可以> 100％，如果调度程序/设置比基线差
  图表是15个单元格的CDF
    [图：示例图]
    对于100％=>坏的权利，会比现在的博格更糟糕
    剩下100％=>好，改进压实状态Borg集群

避免闲置资源的技术
  问：我们请你为讲座点名。让我们听一些！
  抢占
    当需要资源时，启动较低优先级的工作负载
    允许资源被利用，而不是被阻止
    但是：必须准备从抢占中恢复
  大型共享单元格
    不要给不同团队的专用集群
    共享所有机器*大家*
    减少由于低负载而导致的破碎，抑制囤积
  细粒度的资源请求
    允许用户以小单位独立地更改请求（CPU /内存）
    想法：准确的估计可以避免“压力”浪费
    不像AWS EC2实例类型！
  资源估算
    自动将预留缩小到使用周围的安全信封
    [图：示例]
    缓慢衰减可防止由于尖峰引起的重复抢先
  过量使用
    任务可以使用超出限制的资源，有被杀害的危险

扩展：运行超过10k +的机器不是微不足道的（例如，Hadoop YARN：最大5k）
  BorgMaster架构技巧
    在BorgMaster副本上分页
      每个负责一些Borglets（链接碎片）
      参看 实验室4关键范围的分片
    用于Borglet通信和只读RPC的单独线程
    调度器的共享状态设计
      在单独的过程中
      适用于BorgMaster中集群状态的快照
      BorgMaster根据陈旧状态拒绝更新
  调度技巧
    2011年Borg细胞痕迹：150k +任务
      抢占需要重新考虑即使运行
    一切的迭代是令人望而生畏的
      每个任务〜20k个周期（比较：网络数据包接收= 16k）
    等值班
      只能在类似的任务中进行一次可行性检查和评分
    分数缓存
      如果状态没有改变，不要重新计算分数
    轻松随机
      不要迭代整个单元格
      随机抽取候选机器，直到找到足够的
      “pickier”任务还需要更长的时间

任务隔离
  Linux容器（chroot + cgroups）
    内核命名空间（与VM不同，内核仍然共享）
    每个任务都有自己的根文件系统
  包
    包括静态链接任务二进制文件，加上需要的额外文件
    类似于Docker图像
  “Appclasses”定制机器处理任务
    例如，内核调度程序中的CPU量
  性能隔离
    目标：prod优先任务总是得到他们要求的
    可压缩资源：低优先级任务的速率限制
    不可压缩的资源：抢占较低优先级的任务释放

服务发现
  自动化=>用户不知道任务在运行时的位置！
    实验1：如果您不知道其IP /主机，如何查找MR主机？
    实验4：如何找到shardmaster / replica组成员？
  Borg使用Chubby（类似于Zookeeper / lec 8）来存储任务位置
    RPC系统可以在Chubby中查找工作：任务组合
  “博格名称系统”（BNS）
    与集群DNS集成
    50.jfoo.ubar.cc.borg.google.com =任务50，作业“foo”，单元格“cc”
  因此，任何人都可以使RPC只知道cluster +作业名称和任务ID
  这个广泛的问题在工业中被称为“服务发现”
    许多解决方案：Zookeeper，领事，SkyDNS，...
    所有类似于BNS

与其他系统的差异
  YARN（Yahoo! / Microsoft）
    主要用于批量工作负载
    基于槽位的资源模型
    应用程序主人做调度
  Mesos（Twitter w / Aurora，AirBnB w / Marathon）
    Mesos不实现调度（Borg）
    Mesos为特定于应用程序的框架提供资源
      例如MapReduce / Spark调度程序
      框架可以接受或拒绝，等待更好的报价
    实践中的一些挑战（见欧米茄论文）
      信息隐藏（得分棘手）
      资源囤积（缓慢调度问题）
      优先抢占困难（需要提供API的一部分）
  小酒馆/ Tupperware（Facebook）
    小酒馆使用资源森林模型
    试图找到通过森林的路径来预留资源
    对Tupperware很少了解
  Kubernetes
    开源，较小规模（1k个节点）
    不如博格在政策方面那么复杂

博格用户体验
  失败（特别是preemptions！）确实发生了所有的时间
    当“尽力而为”长时间工作时，约5％的MapReduce任务被抢占
    可能在极端情况下饿死，虽然很少见
  资源估算是非常有用的，在实践中效果很好
    10 GB内存预留不足=>请求20 GB，即使需要12 GB


参考文献：
  群集压缩指标：http://research.google.com/pubs/archive/43103.pdf
  Borg，Omega和Kubernetes上的ACM队列文章：
    http://queue.acm.org/detail.cfm?id=2898444
  开源系统：
    https://mesos.apache.org
    https://kubernetes.io