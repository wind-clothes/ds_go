##Gfs

Google文件系统

我们为什么看这篇文章？
  用于映射/缩小的文件系统
  本文主要介绍了6.824
    交易一致性简单和性能
    后续设计的动机
  良好的系统文件 - 从应用程序到网络的细节
    性能，容错性，一致性
  有影响
    许多其他系统使用GFS（例如，bigtable）
    基于GFS的HDFS（Hadoop分布式文件系统）
    
什么是一致性？
  正确的条件
  当应用程序复制和并发访问数据时重要 
    如果一个应用程序执行一个写入，那么以后读取的是什么呢？
      如果读取来自不同的应用程序怎么办？
  弱一致性
    read（）可能会返回陈旧的数据---不是最近写入的结果
  一致性强
    read（）总是从最近的write（）返回数据，
  一般权衡：
    强大的一致性对于应用程序作者很好
    强烈的一致性对性能不利
  许多正确性条件（通常称为一致性模型）
    今天第一高峰; 几乎会在我们读这个词的每一篇文章中出现
    
“理想”一致性模型
  复制的文件的行为类似于非复制文件系统
    图片：同一机器上的许多客户端访问单个磁盘上的文件
  如果一个应用程序写入，稍后的读取将会遵守该写入
  如果两个应用程序同时写入同一个文件怎么办？
    在文件系统中经常未定义---文件可能会有一些混合内容
  如果两个应用程序同时写入同一目录怎么办？
    一个是第一个，另一个是第二个

实现理想一致性的挑战
  并发
  机器故障
  网络分区

为什么这些挑战难以克服：
  需要客户端和服务器之间的通信
    可能性价比
  协议可能变得复杂 - 见下周
    难以正确实施系统
  6.824中的许多系统都不能提供理想
    GFS就是一个例子

全球行动计划的核心挑战：
  有很多机器故障是常见的
    假设机器每年失败一次
    有1000台机器，〜3台每天都会失败。
  高性能：许多并发读者和作家
    映射/减少作业在GFS中读取并存储最终结果
    注意：*不*临时的中间文件
  有效利用网络
  这些难题与“理想”一致性相结合

高级设计
  目录，文件，名称，打开/读/写
    但不是POSIX
  100个具有磁盘的Linux块服务器
    存储64MB块（每个块的普通Linux文件）
    每个组块在三台服务器上复制
    问：为什么3x复制？
    问：除了数据的可用性，3x复制给了我们什么？
       用于读取热文件的负载平衡
       亲和力
    问：为什么不将每个文件的一个副本存储在RAID的磁盘上？
       RAID不是商品
       想要整机容错; 不只是存储设备
    问：为什么大块这么大？
  GFS主服务器知道目录层次结构
    对于目录，它中有哪些文件
    对于文件，知道每个64 MB的块服务器
    主人在记忆中保持状态
      每个块的64个字节的元数据
    master具有用于元数据的私有可恢复数据库
      主机可以从电源故障迅速恢复
    影子大师落后于主人
      可以升职掌握

基本文件操作：
  客户阅读：
    发送文件名和偏移到主
    主回复有一组具有该块的服务器
      响应包括版本＃的块
      客户端缓存该信息
    询问最近的块服务器
      检查版本＃
      如果版本号错误，请重新联系主人
  客户端追加
    问大师在哪里存储
      如果跨越64 MB，大师可能会选择一组新的组块服务器
      主机响应组块服务器和版本＃
        一个组块服务器是主要的
    客户将数据推送到副本
      副本形成一个链
      链承载网络拓扑
      允许快速复制
    当数据在所有的块服务器上时，客户端都联系主
      主要分配序列号
      主要在本地进行更改
      主要转发请求到副本
      在收到所有副本后的ack后，主要响应客户端
    如果一个副本没有响应，则客户端重试
      联系大师后
  如果主人不刷新租约，大师可以任命新的主人
  如果数字副本下降到一些数字以下，主人将复制块
  大师重新平衡复制品

GFS是否达到“理想”的一致性？
  两种情况：目录和文件
  目录：是的，但...
    是的：强一致性（只有一份）
    但：
      船长可能会下坡，GFS不可用
        影子主人可以提供只读操作，这可能会返回陈旧的数据
        问：为什么不写操作？
      裂脑综合征（见下一讲）
  文件：不总是
    原子附加突变
      一个文件可以有重复的条目和孔
        如果主服务器无法联系副本，则主要向客户端报告错误
        客户端重试和主选择一个新的偏移量
    记录可以以两个偏移量重复
    而其他复制品可以在一个偏移处具有孔
    “不幸”客户端可以在短时间内读取过时的数据
      失败的突变使得块不一致
        主块服务器更新块
        但是失败了，复制品已经过时了
      客户端可能会读取不可更新的块
      当客户刷新租约时，会了解新版本＃
    没有原子附加的突变
      几个客户的数据可能混杂
      在非复制Unix上的并发写入也可能导致一个奇怪的结果
      如果是，请使用atomic append或临时文件并进行原子重命名

作者声称弱一致性不是应用程序的一大问题    
  大多数文件更新是仅附加更新
    应用程序可以使用附加记录中的UID来检测重复项
    应用程序可能只读取较少的数据（但不是过时的数据）
  应用程序可以使用临时文件和原子重命名
    
表现（图3）
  巨大的汇总吞吐量（3份，条带）
    总计125 MB /秒
    靠近饱和的网络
  写入低于可能最大值的不同文件
    作者责怪他们的网络堆栈
    它会导致从一个副本传播到下一个副本的延迟
  并发附加到单个文件
    受限于存储最后一个块的服务器
    
概要
  性能，容错性，一致性的案例研究
    专门为MapReduce应用程序
  在GFS中有什么好的？
    巨大的顺序读写
    追加
    巨大的吞吐量（3份，条纹）
    数据容错（3份）
  在航空安全计划中有什么好处？
    主机容错
    小文件（掌握瓶颈）
    客户可能会看到陈旧的数据
    附加可能重复
