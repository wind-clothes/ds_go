## Pub-Sub

Wormhole: Reliable Pub-Sub to support Geo-replicated Internet Services.

Wormhole：可靠的Pub-Sub支持全球复制的互联网服务。

为什么我们读这篇文章？

* 分布式系统中的pub-sub公共构建块：YMB，FAB，Kafka
* 案例研究：Facebook的Wormhole：由memcache驱动

网站如何扩大负载增长？一个典型的演变历史：

* 一台机器，Web服务器，应用程序，DB

数据库存储在磁盘，崩溃恢复，事务，SQL,应用查询DB，格式，HTML和c,但负载增长，您的PHP应用程序花费太多的CPU时间。

* 许多Web FE，一个共享数据库

一个容易的变化，因为Web服务器+应用程序已经与存储分离,FE是无状态的，通过DB共享（并发控制）,但负荷增长; 添加更多FE; 很快单个DB服务器就是瓶颈。

* 许多Web FE，数据进行分库分表到不同的数据库中

通过数据按照某种分区算法分割数据，应用程序查看数据（例如用户），选择正确的数据库，如果没有热点数据，并且有很好的数据库并行性，跨表事务和连表查询可能不起作用
，但是DB相比缓存还是很慢，即使是读，为什么不缓存读请求？

* Web FE集群，用于读取的缓存集群，许多用于写入的DB集群

经济实惠的b/c读取和memcached比DB快10倍，memcached只是一个内存中的哈希表，很简单，复杂的b/c DB和memcached可能会失去同步，（下一个瓶颈将是DB写 - 很难解决）。

大Facebook的基础设施图片

很多用户，朋友列表，状态，帖子，喜欢，照片，新鲜/一致的数据显然不是关键

* 高负载：每秒数十亿次操作，这是一个DB服务器的吞吐量的10,000倍
* 多个数据中心（至少西海岸和东海岸）每个数据中心 - “区域”：“真实”数据通过MySQL数据库分片，memcached层（mc），web服务器（memcached的客户端），每个数据中心的数据库都包含完整的副本，西海岸是Master，其他人是通过MySQL异步日志复制的replication

FB在mc中存储什么？

也许userID  - > name; userID  - >朋友列表; postID  - > text; 网址 - >喜欢;基本上从DB复制数据

FB应用程序如何使用mc？
```

	读：
    v = get（k）（计算哈希（k）以选择mc服务器）
    如果v为零{
      v =从本地DB获取
      set（k，v）
    }
    写：
    v =新值
    发送k，v到主DB＃可能在偏远地区

```
如何处理不同地区的数据库更新？
  主数据库接收所有写入（类似于PNUTS）
  将数据添加到事务日志
  将事务日志复制到从站

如何安排mc在不同地区了解写作

* 需要在写入后无效/更新mc条目：eval部分建议避免过时的数据很重要
* 选项1：远程区域的mc轮询其本地DB：增加DB上的读取负载

发布/订阅

Facebook用例，用户是mc，发布商是DB；订阅者链接我们的系统，更新配置文件以表示对更新的兴趣，储存在ZK中；发布商阅读配置文件以查找订阅者，与每个用户建立流，每个流异步地发送更新，一组键值对；过滤器，订阅者告诉发布商有关过滤器，过滤器是在蠕虫孔更新中的密钥查询，发布商只发送通过过滤器的更新。

传递语义

* 流量上的所有更新按顺序交付，发布者维护每个用户一个“数据标记”，事务日志中更新的序列号，记录订阅者收到的内容；发布商定期询问订阅者收到的内容，即，标记是用户已经收到的下限；更新至少交付一次，发布商持续标记，如果发布商无法从上一个标记开始发送，=>订阅者可能会收到两次更新；问：订户如何处理多次提供的更新，A：缓存没问题，答：应用程序可以进行重复过滤；问：是不是可以更新？答：是的，因为事务日志可能已被截断，日志中的数据存在1-2天。

在哪里存放标记？

* SCRD：发布商存储在本地持久存储中

如果存储不可用，则无法将故障切换到新的发布者，缓存将会过时

如果存储失败，丢失标记，机会：存储/日志经常被复制

* MCRD：发行商在Zookeeper中存储标记

如果发布商失败，另一个发布商可以接管，从zookeeper读取最后一个标记

实施挑战：标记格式
  相同日志的副本具有不同的二进制格式
  解决方案：“逻辑”的立场

* 问：在Zookeeper中更新标记不是很贵吗？
    答：是的，但只定期进行

实施挑战：许多不同的DB

不想修改任何一个来支持流，想法：生产者读取DB的事务日志，读库读取不同的日志格式
，以标准格式转换更新，一个键是分片标识符。

优化1：caravan

设计1：每一个读者一个流，对DB负载过大，在稳定状态下，所有读者都读取相同的更新数据

设计2：一个读者为所有流，恢复性能不佳，在恢复每个流程可能必须从日志中的不同点读取

解决方案：每个流的一个读者（“大篷车”），在实践中，大篷车的数量很少（〜1），一个大篷车叫做“caravan”
    
优化2：负载平衡流

单个应用程序有几个订阅者，应用程序数据也被分解，N DB分片 M应用机；- > MC机器可能有多于1个用户。

两个计划：- 加权随机选择- 订阅者使用zookeeper

优化3：一个TCP连接
  为同一用户复用多个流
  用户可能有几个分片
  每个分片的一个流程
